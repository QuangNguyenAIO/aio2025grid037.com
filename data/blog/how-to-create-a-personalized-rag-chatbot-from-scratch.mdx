---
title: How to Create a Personalized RAG Chatbot from Scratch
date: 2025-7-4
tags: ['python', 'AI', 'chatbot', 'RAG', 'transformers', 'streamlit', 'langchain', 'machine learning', 'deeplearning' ]
summary: BÃ i viáº¿t nÃ y cung cáº¥p hÆ°á»›ng dáº«n chi tiáº¿t cÃ¡ch táº¡o vÃ  triá»ƒn khai má»™t chatbot RAG cÃ¡ nhÃ¢n hÃ³a báº±ng Python, sá»­ dá»¥ng cÃ¡c thÆ° viá»‡n nhÆ° Transformers, LangChain vÃ  giao diá»‡n trá»±c quan vá»›i Streamlit.
---

import Twemoji from './components/ui/Twemoji';

## Táº¡i sao láº¡i lÃ  RAG?

- Giá»›i thiá»‡u khÃ¡i niá»‡m **Retrieval-Augmented Generation (RAG)**.
  - Váº¥n Ä‘á» cá»§a LLMs khi khÃ´ng cÃ³ thÃ´ng tin thá»i gian thá»±c hoáº·c dá»¯ liá»‡u cá»¥ thá»ƒ.
  - Giáº£i phÃ¡p RAG: káº¿t há»£p kháº£ nÄƒng sinh vÄƒn báº£n cá»§a LLM vá»›i há»‡ thá»‘ng truy xuáº¥t dá»¯ liá»‡u tá»« nguá»“n bÃªn ngoÃ i.
- á»¨ng dá»¥ng thá»±c táº¿ cá»§a RAG: chatbot chÄƒm sÃ³c khÃ¡ch hÃ ng, tÆ° váº¥n kiáº¿n thá»©c chuyÃªn ngÃ nh, há»‡ thá»‘ng ná»™i bá»™ doanh nghiá»‡p,...
- Trong project nÃ y, má»¥c tiÃªu lÃ  táº¡o má»™t chatbot cÃ³ kháº£ nÄƒng tráº£ lá»i cÃ¡c cÃ¢u há»i xoay quanh má»™t chá»§ Ä‘á» cÃ¡ nhÃ¢n (vÃ­ dá»¥: blog cá»§a báº¡n, sáº£n pháº©m cÃ¡ nhÃ¢n, kiáº¿n thá»©c chuyÃªn sÃ¢u,...)

---

## Tá»•ng quan dá»± Ã¡n

- MÃ´ táº£ luá»“ng hoáº¡t Ä‘á»™ng tá»•ng thá»ƒ cá»§a chatbot RAG:
  1. Nháº­n cÃ¢u há»i tá»« ngÆ°á»i dÃ¹ng
  2. DÃ¹ng embedding Ä‘á»ƒ tÃ¬m cÃ¡c Ä‘oáº¡n vÄƒn báº£n liÃªn quan trong táº­p dá»¯ liá»‡u cÃ¡ nhÃ¢n
  3. ÄÆ°a káº¿t quáº£ truy xuáº¥t + cÃ¢u há»i vÃ o LLM Ä‘á»ƒ táº¡o cÃ¢u tráº£ lá»i
- Giá»›i thiá»‡u cÃ´ng nghá»‡ sá»­ dá»¥ng:
  - Python
  - LangChain (quáº£n lÃ½ pipeline RAG)
  - Hugging Face Transformers (mÃ´ hÃ¬nh ngÃ´n ngá»¯ + embeddings)
  - FAISS hoáº·c ChromaDB (vector store)
  - Streamlit (táº¡o giao diá»‡n web Ä‘Æ¡n giáº£n)

---

## CÃ¡c bÆ°á»›c xÃ¢y dá»±ng dá»± Ã¡n

### BÆ°á»›c 1: CÃ i Ä‘áº·t mÃ´i trÆ°á»ng vÃ  thÆ° viá»‡n cáº§n thiáº¿t

- **Äá»‘i vá»›i Google Colab**: Náº¿u báº¡n sá»­ dá»¥ng Google Colab, chá»‰ cáº§n cháº¡y cÃ¡c lá»‡nh sau Ä‘á»ƒ cÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t:
  ```bash
  !pip install -q transformers==4.52.4
  !pip install -q bitsandbytes==0.46.0
  !pip install -q accelerate==1.7.0
  !pip install -q langchain==0.3.25
  !pip install -q langchainhub==0.1.21
  !pip install -q langchain-chroma==0.2.4
  !pip install -q langchain_experimental==0.3.4
  !pip install -q langchain-community==0.3.24
  !pip install -q langchain_huggingface==0.2.0
  !pip install -q python-dotenv==1.1.0
  !pip install -q pypdf
  ```

- **Äá»‘i vá»›i mÃ´i trÆ°á»ng local**: Náº¿u báº¡n cháº¡y trÃªn mÃ¡y local, cáº§n thiáº¿t láº­p mÃ´i trÆ°á»ng Conda trÆ°á»›c. DÆ°á»›i Ä‘Ã¢y lÃ  má»™t file `environment.yaml` máº«u Ä‘á»ƒ táº¡o mÃ´i trÆ°á»ng:
  ```yaml
  name: myenv
  channels:
    - pytorch
    - nvidia
    - conda-forge
    - defaults
  dependencies:
    - python=3.12
    - numpy
    - pytorch=2.2.2
    - pytorch-cuda=11.8
  ```
  Äá»ƒ táº¡o mÃ´i trÆ°á»ng tá»« file YAML nÃ y, lÆ°u ná»™i dung vÃ o file `environment.yaml`, sau Ä‘Ã³ cháº¡y lá»‡nh:
  ```bash
  conda env create -f environment.yaml
  ```
  Sau khi mÃ´i trÆ°á»ng Ä‘Æ°á»£c táº¡o, kÃ­ch hoáº¡t mÃ´i trÆ°á»ng báº±ng lá»‡nh:
  ```bash
  conda activate myenv
  ```
  Tiáº¿p theo, cÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t báº±ng lá»‡nh:
  ```bash
  pip install -r requirements.txt
  ```
  Ná»™i dung file `requirements.txt` nÃªn bao gá»“m:
  ```
  transformers==4.52.4
  bitsandbytes==0.46.0
  accelerate==1.7.0
  langchain==0.3.25
  langchainhub==0.1.21
  langchain-chroma==0.2.4
  langchain_experimental==0.3.4
  langchain-community==0.3.24
  langchain_huggingface==0.2.0
  python-dotenv==1.1.0
  pypdf
  ```

Sau khi cÃ i Ä‘áº·t xong mÃ´i trÆ°á»ng thÃ¬ **xin chÃºc má»«ng báº¡n** Ä‘Ã£ vÆ°á»£t qua cá»­a áº£i Ä‘áº§u tiÃªn.

![ChÃºc má»«ng báº¡n!](/static/images/blogs/blender-anya.png)

### BÆ°á»›c 2: XÃ¢y dá»±ng há»‡ thá»‘ng lÆ°u trá»¯ cÃ¡c vector embedding

- **Import cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t**: Äáº§u tiÃªn, báº¡n cáº§n import cÃ¡c thÆ° viá»‡n Ä‘á»ƒ xÃ¢y dá»±ng há»‡ thá»‘ng:
  ```python
  import torch
  from transformers import BitsAndBytesConfig
  from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
  from langchain_huggingface import HuggingFaceEmbeddings
  from langchain_huggingface.llms import HuggingFacePipeline
  from langchain.memory import ConversationBufferMemory
  from langchain_community.chat_message_histories import ChatMessageHistory
  from langchain_community.document_loaders import PyPDFLoader, TextLoader
  from langchain.chains import ConversationalRetrievalChain
  from langchain_experimental.text_splitter import SemanticChunker
  from langchain_chroma import Chroma
  from langchain_text_splitters import RecursiveCharacterTextSplitter
  from langchain_core.runnables import RunnablePassthrough
  from langchain_core.output_parsers import StrOutputParser
  from langchain import hub
  ```

- **Äá»c file PDF**: Báº¡n hÃ£y chuáº©n bá»‹ vÃ  upload má»™t file PDF táº§m 20 trang trá»Ÿ lÃªn Ä‘á»ƒ test khi xÃ¢y dá»±ng há»‡ thá»‘ng. Sau Ä‘Ã³, sá»­ dá»¥ng class `PyPDFLoader` Ä‘á»ƒ Ä‘á»c file PDF nÃ y lÃªn nhÆ° sau:
  ```python
  Loader = PyPDFLoader
  FILE_PATH = "ten_file_cua_ban.pdf"
  loader = Loader(FILE_PATH)
  documents = loader.load()
  ```
- **Embedding model**: LÃ  mÃ´ hÃ¬nh giÃºp vector hÃ³a cÃ¡c vÄƒn báº£n nháº±n thá»±c hiá»‡n viá»‡c tÃ­nh toÃ¡n vÃ  lÆ°u trá»¯, dÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡ch sá»­ dá»¥ng HuggingFaceEmbeddings vá»›i mÃ´ hÃ¬nh bkai-foundation-models/vietnamese-bi-encoder Ä‘Æ°á»£c train Ä‘á»ƒ hiá»ƒu ngá»¯ nghÄ©a vá»›i tiáº¿ng Viá»‡t.

  ```python
  EMBEDDING_MODEL_NAME = "bkai-foundation-models/vietnamese-bi-encoder"
  embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
  ```
- **Chunking**: TÃ¡ch vÄƒn báº£n dÃ i thÃ nh cÃ¡c Ä‘oáº¡n ngáº¯n hÆ¡n thay vÃ¬ sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p truyá»n thá»‘ng chá»‰ tÃ¡ch cÃ¡c vÄƒn báº£n theo Ä‘á»™ dÃ i thÃ¬ mÃ¬nh sáº½ sá»­ dá»¥ng ká»¹ thuáº­t **Semantic Chunking** Ä‘á»ƒ tÃ¡ch nhÆ°ng sáº½ dá»±a vÃ o má»©c Ä‘á»™ ngá»¯ nghÄ©a Ä‘á»ƒ trÃ¡nh lÃ m vá»¡ máº¡ch vÄƒn vÃ  chia nhá» cÃ¡c Ä‘oáº¡n cÃ³ cÃ¹ng má»™t chá»§ Ä‘á». DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡ch sá»­ dá»¥ng SemanticChunker Ä‘á»ƒ triá»ƒn khai:
![chunking!](/static/images/blogs/rag-chatbot/Slide1.PNG)
  ```python
  semantic_splitter = SemanticChunker(
    embeddings=embeddings,
    buffer_size=1,
    breakpoint_threshold_type="percentile",
    breakpoint_threshold_amount=95,
    min_chunk_size=500,
    add_start_index=True
  )
  ```
  Giáº£i thÃ­ch cÃ¡c tham sá»‘:
  - `embeddings`: ÄÃ¢y lÃ  mÃ´ hÃ¬nh embedding Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh toÃ¡n ngá»¯ nghÄ©a cá»§a vÄƒn báº£n. MÃ´ hÃ¬nh nÃ y giÃºp xÃ¡c Ä‘á»‹nh má»©c Ä‘á»™ liÃªn quan vá» máº·t ná»™i dung giá»¯a cÃ¡c cÃ¢u hoáº·c Ä‘oáº¡n vÄƒn, tá»« Ä‘Ã³ há»— trá»£ viá»‡c tÃ¡ch Ä‘oáº¡n má»™t cÃ¡ch chÃ­nh xÃ¡c hÆ¡n.
  - `buffer_size`: Tham sá»‘ nÃ y xÃ¡c Ä‘á»‹nh sá»‘ lÆ°á»£ng cÃ¢u hoáº·c Ä‘oáº¡n vÄƒn Ä‘Æ°á»£c giá»¯ trong bá»™ Ä‘á»‡m Ä‘á»ƒ phÃ¢n tÃ­ch ngá»¯ nghÄ©a. Vá»›i giÃ¡ trá»‹ `1` cÃ³ nghÄ©a lÃ  1 nhÃ³m gá»“m 1 cÃ¢u.
  - `breakpoint_threshold_type`: ÄÃ¢y lÃ  loáº¡i ngÆ°á»¡ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xÃ¡c Ä‘á»‹nh Ä‘iá»ƒm cáº¯t giá»¯a cÃ¡c Ä‘oáº¡n. GiÃ¡ trá»‹ `"percentile"` cho biáº¿t ngÆ°á»¡ng Ä‘Æ°á»£c tÃ­nh dá»±a trÃªn tá»‰ lá»‡ pháº§n trÄƒm cá»§a sá»± khÃ¡c biá»‡t ngá»¯ nghÄ©a giá»¯a cÃ¡c cÃ¢u hoáº·c Ä‘oáº¡n.
  - `breakpoint_threshold_amount`: Vá»›i giÃ¡ trá»‹ `95`, ngÆ°á»¡ng cáº¯t Ä‘Æ°á»£c Ä‘áº·t á»Ÿ phÃ¢n vá»‹ thá»© 95. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  chá»‰ khi sá»± khÃ¡c biá»‡t vá» ngá»¯ nghÄ©a giá»¯a hai cÃ¢u hoáº·c Ä‘oáº¡n vÆ°á»£t quÃ¡ 95% cÃ¡c giÃ¡ trá»‹ khÃ¡c biá»‡t khÃ¡c thÃ¬ Ä‘iá»ƒm cáº¯t má»›i Ä‘Æ°á»£c táº¡o.
  - `min_chunk_size`: ÄÃ¢y lÃ  kÃ­ch thÆ°á»›c tá»‘i thiá»ƒu cá»§a má»™t Ä‘oáº¡n vÄƒn sau khi cáº¯t, Ä‘Æ°á»£c Ä‘áº·t lÃ  `500` kÃ½ tá»±. Tham sá»‘ nÃ y Ä‘áº£m báº£o ráº±ng cÃ¡c Ä‘oáº¡n khÃ´ng bá»‹ chia quÃ¡ nhá», giÃºp duy trÃ¬ Ã½ nghÄ©a vÃ  máº¡ch vÄƒn.
  - `add_start_index`: Khi Ä‘Æ°á»£c Ä‘áº·t lÃ  `True`, tham sá»‘ nÃ y thÃªm chá»‰ sá»‘ báº¯t Ä‘áº§u vÃ o metadata cá»§a má»—i Ä‘oáº¡n. Äiá»u nÃ y há»¯u Ã­ch Ä‘á»ƒ theo dÃµi vá»‹ trÃ­ cá»§a Ä‘oáº¡n trong vÄƒn báº£n gá»‘c, Ä‘áº·c biá»‡t khi cáº§n tham chiáº¿u ngÆ°á»£c láº¡i.
  - Thá»±c hiá»‡n chia tÃ i liá»‡u:
  ```python
  documents = semantic_splitter.split_documents(documents)
  ```
- **Vector database**: Nháº±m lÆ°u trá»¯ cÃ¡c chunk á»Ÿ dÆ°á»›i dáº¡ng cÃ¡c vector phá»¥c vá»¥ trong quÃ¡ trÃ¬nh truy váº¥n. DÃ¹ng `Chroma`, **embedding model** vÃ  input lÃ  **documents** Ä‘á»ƒ khá»Ÿi táº¡o vector database.
  ```python
  # khá»Ÿi táº¡o vector database
  vec_db = Chroma.from_documents(documents=documents, embedding=embeddings)
  # giÃºp thá»±c hiá»‡n truy váº¥n
  retriever = vec_db.as_retriever() 
  ```
  - CÃ¡ch thá»©c truy váº¥n: 
  ```python
  result = retriever.invoke('Anya-chan lÃ  ai?')
  # Tráº£ vá» 1 list cÃ¡c tÃ i liá»‡u cÃ³ liÃªn quan Ä‘áº¿n prompt
  ``` 

### BÆ°á»›c 3: Káº¿t ná»‘i vá»›i mÃ´ hÃ¬nh LLM
- **Sá»­ dá»¥ng model**: trong dá»¥ Ã¡n cá»§a mÃ¬nh mÃ¬nh dÃ¹ng model [openchat-3.5-0106](https://huggingface.co/openchat/openchat-3.5-0106) lÃ m LLM chÃ­nh cho chatbot.
![logo-openchat!](/static/images/blogs/rag-chatbot/logo_new.png)
- Khá»Ÿi táº¡o model:
```python
  # config giÃºp load model tá»‘t hÆ¡n
  nf4_config = BitsAndBytesConfig(
  load_in_4bit=True,
  bnb_4bit_quant_type="nf4",
  bnb_4bit_use_double_quant=True,
  bnb_4bit_compute_dtype=torch.bfloat16
  )
  # model vÃ  tokenizer
  MODEL_NAME = "openchat/openchat-3.5-0106"
  tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
  model = AutoModelForCausalLM.from_pretrained(MODEL_NAME,
                                              quantization_config=nf4_config,
                                              low_cpu_mem_usage=True
  )
```
- Táº¡o 1 pipeline Ä‘á»ƒ sá»­ dá»¥ng:
```python
pipeline = pipeline(
  "text-generation",
  model=model,
  tokenizer=tokenizer,
  max_new_tokens=256,
  device_map="auto",
  pad_token_id=tokenizer.eos_token_id,
)
llm = HungingFacePipeline(pipeline=pipeline)
```
- Káº¿t há»£p láº¡i vá»›i LangChain:
```python
# KÃ©o prompt tá»« hub
prompt = hub.pull("rlm/rag-prompt")

# HÃ m Ä‘á»‹nh dáº¡ng tÃ i liá»‡u Ä‘á»ƒ sá»­ dá»¥ng trong chuá»—i
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# XÃ¢y dá»±ng chuá»—i RAG
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# CÃ¡ch cháº¡y chuá»—i RAG Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i
result = rag_chain.invoke("Anya-chan lÃ  ai?")
print(result)
```
- **Giáº£i thÃ­ch Ä‘oáº¡n mÃ£**:
  - `prompt = hub.pull("rlm/rag-prompt")`: KÃ©o má»™t prompt Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a sáºµn tá»« LangChain Hub cho cÃ¡c á»©ng dá»¥ng RAG.
  - `format_docs`: Äá»‹nh dáº¡ng cÃ¡c tÃ i liá»‡u Ä‘Æ°á»£c truy xuáº¥t thÃ nh má»™t chuá»—i vÄƒn báº£n duy nháº¥t.
  - `rag_chain`: Káº¿t há»£p cÃ¡c thÃ nh pháº§n Ä‘á»ƒ truy xuáº¥t tÃ i liá»‡u, Ä‘á»‹nh dáº¡ng chÃºng, sá»­ dá»¥ng prompt, gá»­i Ä‘áº¿n LLM vÃ  chuyá»ƒn Ä‘á»•i Ä‘áº§u ra thÃ nh chuá»—i vÄƒn báº£n.
  - `rag_chain.invoke()`: Cháº¡y chuá»—i vá»›i má»™t cÃ¢u há»i cá»¥ thá»ƒ vÃ  tráº£ vá» cÃ¢u tráº£ lá»i tá»« LLM dá»±a trÃªn ngá»¯ cáº£nh Ä‘Æ°á»£c truy xuáº¥t.

- **ğŸŠChÃºc má»«ng! Báº¡n vá»«a Ä‘áº¡t Ä‘Æ°á»£c má»™t bÆ°á»›c tiáº¿n má»›iğŸ‰** (Má»Ÿ khÃ³a ká»¹ nÄƒng má»›i - Táº­p trung liÃªn tá»¥c)
![chuc-mung-báº¡n!](/static/images/blogs/anya-chan-chuc-mung.gif)

### BÆ°á»›c 4: XÃ¢y dá»±ng giao diá»‡n vá»›i Streamlit

- **Streamlit lÃ  gÃ¬?**: ÄÃ¢y lÃ  thÆ° viá»‡n Python giÃºp táº¡o giao diá»‡n web tÆ°Æ¡ng tÃ¡c nhanh chÃ³ng. Vá»›i chatbot RAG, Streamlit sáº½ há»— trá»£ xÃ¢y dá»±ng giao diá»‡n Ä‘á»ƒ upload file PDF, hiá»ƒn thá»‹ lá»‹ch sá»­ chat vÃ  nháº­p cÃ¢u há»i.
- **Cáº¥u trÃºc cÆ¡ báº£n**: Giao diá»‡n gá»“m sidebar Ä‘á»ƒ cÃ i Ä‘áº·t vÃ  upload file, cÃ¹ng khu vá»±c chÃ­nh hiá»ƒn thá»‹ cuá»™c trÃ² chuyá»‡n.
- **Khá»Ÿi táº¡o tráº¡ng thÃ¡i**: Sá»­ dá»¥ng `st.session_state` Ä‘á»ƒ lÆ°u trá»¯ thÃ´ng tin nhÆ° lá»‹ch sá»­ chat vÃ  tráº¡ng thÃ¡i mÃ´ hÃ¬nh:
  ```python
  if 'rag_chain' not in st.session_state:
      st.session_state.rag_chain = None
  if 'chat_history' not in st.session_state:
      st.session_state.chat_history = []
  if 'pdf_processed' not in st.session_state:
      st.session_state.pdf_processed = False
  ```
- **Táº£i mÃ´ hÃ¬nh hiá»‡u quáº£**: DÃ¹ng `@st.cache_resource` Ä‘á»ƒ trÃ¡nh táº£i láº¡i mÃ´ hÃ¬nh má»—i láº§n á»©ng dá»¥ng rerun:
  ```python
  @st.cache_resource
  def load_embeddings():
      return HuggingFaceEmbeddings(
          "bkai-foundation-models/vietnamese-bi-encoder"
      )

  @st.cache_resource
  def load_llm():
      bnb_config = BitsAndBytesConfig(
          load_in_4bit=True, 
          bnb_4bit_quant_type="nf4"
      )
      model = AutoModelForCausalLM.from_pretrained(
          "openchat/openchat-3.5-0106", 
          quantization_config=bnb_config
      )
      tokenizer = AutoTokenizer.from_pretrained(
          "openchat/openchat-3.5-0106"
      )
      return HuggingFacePipeline(
          pipeline=pipeline(
              "text-generation", 
              model=model, 
              tokenizer=tokenizer, 
              max_new_tokens=512
          )
      )
  ```
- **Xá»­ lÃ½ PDF**: HÃ m `process_pdf` Ä‘á»ƒ Ä‘á»c file, chia nhá» vÃ  táº¡o chuá»—i RAG:
  ```python
  def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)
  def process_pdf(uploaded):
      loader = PyPDFLoader(uploaded)
      docs = loader.load()
      semantic_splitter = SemanticChunker(
          embeddings=st.session_state.embeddings
      )
      docs = semantic_splitter.split_documents(docs)
      vector_db = Chroma.from_documents(
          docs, 
          embedding=st.session_state.embeddings
      )
      retriever = vector_db.as_retriever()
      rag_chain = (
          {"context": retriever | format_docs, 
           "question": RunnablePassthrough()} 
          | hub.pull("rlm/rag-prompt") 
          | st.session_state.llm 
          | StrOutputParser()
      )
      return rag_chain, len(docs)
  ```
- **Giao diá»‡n chÃ­nh**: Thiáº¿t láº­p bá»‘ cá»¥c vá»›i sidebar vÃ  khu vá»±c chat:
  ```python
  def main():
      st.set_page_config(
          page_title="PDF RAG Chatbot", 
          layout="wide"
      )
      st.title("PDF RAG Assistant")
      with st.sidebar:
          st.title("âš™ï¸ CÃ i Ä‘áº·t")
          if st.button("ğŸ”„ Xá»­ lÃ½ PDF"):
              st.session_state.rag_chain, num_chunks = process_pdf(uploaded)
              st.session_state.pdf_processed = True
          st.subheader("ğŸ“‹ HÆ°á»›ng dáº«n")
          st.markdown("1. Upload PDF\n2. Äáº·t cÃ¢u há»i\n3. Nháº­n tráº£ lá»i")
      user_input = st.chat_input("Nháº­p cÃ¢u há»i...")
      if user_input and st.session_state.pdf_processed:
          with st.chat_message("assistant"):
              output = st.session_state.rag_chain.invoke(user_input)
              st.write(output)
  ```
- **TÃ³m táº¯t**: Giao diá»‡n Streamlit giÃºp ngÆ°á»i dÃ¹ng dá»… dÃ ng tÆ°Æ¡ng tÃ¡c vá»›i chatbot, upload tÃ i liá»‡u vÃ  nháº­n cÃ¢u tráº£ lá»i dá»±a trÃªn ná»™i dung PDF.

## Summary

- **TÃ³m táº¯t láº¡i quÃ¡ trÃ¬nh**:
  - Thu tháº­p dá»¯ liá»‡u â†’ táº¡o embedding â†’ truy xuáº¥t vÄƒn báº£n â†’ sinh cÃ¢u tráº£ lá»i â†’ xÃ¢y dá»±ng giao diá»‡n vá»›i Streamlit.
- **Nhá»¯ng cÃ´ng nghá»‡ Ä‘Ã£ há»c Ä‘Æ°á»£c qua bÃ i**:
  - LangChain Ä‘á»ƒ quáº£n lÃ½ pipeline RAG.
  - Vector DB (Chroma) Ä‘á»ƒ lÆ°u trá»¯ vÃ  truy xuáº¥t dá»¯ liá»‡u.
  - Transformers Ä‘á»ƒ sá»­ dá»¥ng mÃ´ hÃ¬nh ngÃ´n ngá»¯ vÃ  embeddings.
  - Streamlit Ä‘á»ƒ táº¡o giao diá»‡n web tÆ°Æ¡ng tÃ¡c.
- **Summary vá» á»©ng dá»¥ng Streamlit**:
  - á»¨ng dá»¥ng Streamlit cung cáº¥p má»™t giao diá»‡n thÃ¢n thiá»‡n Ä‘á»ƒ tÆ°Æ¡ng tÃ¡c vá»›i chatbot RAG, cho phÃ©p ngÆ°á»i dÃ¹ng upload file PDF, Ä‘áº·t cÃ¢u há»i vÃ  nháº­n cÃ¢u tráº£ lá»i dá»±a trÃªn ná»™i dung tÃ i liá»‡u. á»¨ng dá»¥ng tÃ­ch há»£p cÃ¡c tÃ­nh nÄƒng nhÆ° quáº£n lÃ½ lá»‹ch sá»­ chat, xá»­ lÃ½ tÃ i liá»‡u theo thá»i gian thá»±c vÃ  pháº£n há»“i tá»« AI.
---

## ğŸ‰ ChÃºc má»«ng báº¡n Ä‘Ã£ Ä‘á»c xong bÃ i viáº¿t!
![ChÃºc má»«ng báº¡n Ä‘Ã£ Ä‘á»c xong!](/static/images/blogs/anya-chucmung.png)
