---
title: Giáº£i mÃ£ "Attention Is All You Need" vÃ  Triá»ƒn khai Transformer tá»« Ä‘áº§u vá»›i PyTorch
date: 2025-07-10
tags: ['python', 'AI', 'NLP', 'pytorch', 'transformer', 'deeplearning', 'paper-review']
summary: CÃ¹ng nhau "má»• xáº»" kiáº¿n trÃºc Transformer huyá»n thoáº¡i tá»« bÃ i bÃ¡o "Attention Is All You Need" vÃ  tá»± tay code láº¡i báº±ng PyTorch.
---

import Twemoji from './components/ui/Twemoji';

## Sá»± ra Ä‘á»i

### Bá»‘i cáº£nh lá»‹ch sá»­
TrÆ°á»›c nÄƒm 2017, tháº¿ giá»›i Xá»­ lÃ½ NgÃ´n ngá»¯ Tá»± nhiÃªn (NLP) bá»‹ "thá»‘ng trá»‹" bá»Ÿi cÃ¡c máº¡ng nÆ¡-ron há»“i quy nhÆ° **RNN**, **LSTM** vÃ  **GRU**. Máº·c dÃ¹ hiá»‡u quáº£, chÃºng tá»“n táº¡i nhá»¯ng nhÆ°á»£c Ä‘iá»ƒm chÃ­ máº¡ng:
* **Xá»­ lÃ½ tuáº§n tá»± (Sequential processing):** Tá»« nÃ y pháº£i Ä‘á»£i tá»« kia xá»­ lÃ½ xong, khÃ´ng thá»ƒ tÃ­nh toÃ¡n song song.
* **Váº¥n Ä‘á» phá»¥ thuá»™c xa (Long-range dependencies):** KhÃ³ khÄƒn trong viá»‡c ghi nhá»› thÃ´ng tin khi cÃ¢u quÃ¡ dÃ i.

BÃ i bÃ¡o **"Attention Is All You Need"** cá»§a Google Brain (Vaswani et al., 2017) ra Ä‘á»i vá»›i tuyÃªn bá»‘: khÃ´ng cáº§n RNN, chá»‰ cáº§n cÆ¡ cháº¿ **Attention** lÃ  Ä‘á»§.

> **Káº¿t quáº£:** Transformer ra Ä‘á»i, má»Ÿ Ä‘Æ°á»ng cho ká»· nguyÃªn cá»§a BERT, GPT-3, GPT-4 vÃ  cÃ¡c mÃ´ hÃ¬nh GenAI hiá»‡n Ä‘áº¡i ngÃ y nay.

Trong bÃ i viáº¿t nÃ y, chÃºng ta sáº½ Ä‘i sÃ¢u vÃ o kiáº¿n trÃºc vÃ  code láº¡i nÃ³ báº±ng `torch`.

---

## Kiáº¿n trÃºc tá»•ng quan

MÃ´ hÃ¬nh Transformer Ä‘Æ°á»£c thiáº¿t káº¿ vá»›i kiáº¿n trÃºc **Encoder-Decoder** song song hoÃ n toÃ n, cho phÃ©p xá»­ lÃ½ dá»¯ liá»‡u má»™t cÃ¡ch hiá»‡u quáº£ trÃªn GPU vÃ  TPU. KhÃ´ng giá»‘ng nhÆ° RNN pháº£i xá»­ lÃ½ tuáº§n tá»± tá»«ng token, Transformer cÃ³ thá»ƒ xá»­ lÃ½ toÃ n bá»™ sequence cÃ¹ng lÃºc.

### Luá»“ng hoáº¡t Ä‘á»™ng chÃ­nh:

1. **Encoder (MÃ£ hÃ³a):** Nháº­n Ä‘áº§u vÃ o (vÃ­ dá»¥: cÃ¢u tiáº¿ng Anh) vÃ  chuyá»ƒn Ä‘á»•i thÃ nh cÃ¡c vector biá»ƒu diá»…n ngá»¯ nghÄ©a Ä‘a chiá»u. Encoder há»c cÃ¡ch náº¯m báº¯t Ã½ nghÄ©a vÃ  má»‘i quan há»‡ giá»¯a cÃ¡c tá»« trong cÃ¢u Ä‘áº§u vÃ o.

2. **Decoder (Giáº£i mÃ£):** Sá»­ dá»¥ng vector tá»« Encoder Ä‘á»ƒ sinh ra Ä‘áº§u ra (vÃ­ dá»¥: cÃ¢u tiáº¿ng Viá»‡t). Decoder hoáº¡t Ä‘á»™ng theo cÆ¡ cháº¿ tá»± há»“i quy (auto-regressive), dá»± Ä‘oÃ¡n tá»«ng token má»™t cÃ¡ch tuáº§n tá»± dá»±a trÃªn cÃ¡c token Ä‘Ã£ sinh trÆ°á»›c Ä‘Ã³.

### Æ¯u Ä‘iá»ƒm chÃ­nh:
- **TÃ­nh song song cao:** KhÃ´ng cÃ³ phá»¥ thuá»™c tuáº§n tá»± nhÆ° RNN
- **Xá»­ lÃ½ long-range dependencies tá»‘t:** Attention mechanism cho phÃ©p chÃº Ã½ trá»±c tiáº¿p Ä‘áº¿n báº¥t ká»³ token nÃ o
- **Scalable:** Dá»… dÃ ng má»Ÿ rá»™ng vá»›i nhiá»u layers vÃ  heads

### CÃ i Ä‘áº·t thÆ° viá»‡n

Äá»ƒ triá»ƒn khai Transformer tá»« Ä‘áº§u, chÃºng ta cáº§n cÃ i Ä‘áº·t PyTorch vÃ  cÃ¡c thÆ° viá»‡n liÃªn quan. CÃ³ hai cÃ¡ch chÃ­nh Ä‘á»ƒ cÃ i Ä‘áº·t:

#### CÃ¡ch 1: Sá»­ dá»¥ng Conda (Khuyáº¿n nghá»‹)

Táº¡o environment má»›i vá»›i file `environment.yml`:

```yaml
name: torch312_cuda118
channels:
  - pytorch
  - nvidia
  - conda-forge
dependencies:
  - python=3.12
  - pytorch=2.5.1
  - pytorch-cuda=11.8
  - torchvision
  - torchaudio
  - numpy=2.3.5
  - scikit-learn=1.7.2
  - matplotlib
  - pandas=2.3.3
  - pip
  - pip:
      - seaborn
```

**HÆ°á»›ng dáº«n cÃ i Ä‘áº·t:**

```bash
# Táº¡o environment tá»« file yaml
conda env create -f environment.yml

# KÃ­ch hoáº¡t environment
conda activate torch312_cuda118

# Kiá»ƒm tra cÃ i Ä‘áº·t
python -c "import torch; print('PyTorch version:', torch.__version__); print('CUDA available:', torch.cuda.is_available())"
```

#### CÃ¡ch 2: Sá»­ dá»¥ng pip

Náº¿u báº¡n thÃ­ch sá»­ dá»¥ng pip trá»±c tiáº¿p:

```bash
pip install torch torchvision torchaudio
```

Trong code, chÃºng ta sáº½ sá»­ dá»¥ng:
- `torch` vÃ  `torch.nn`: Framework deep learning chÃ­nh
- `math`: Cho cÃ¡c phÃ©p tÃ­nh toÃ¡n há»c (sin, cos, sqrt, etc.)

**LÆ°u Ã½:** PyTorch Ä‘Ã£ tÃ­ch há»£p sáºµn nhiá»u operations cáº§n thiáº¿t, nÃªn chÃºng ta khÃ´ng cáº§n thÃªm thÆ° viá»‡n nÃ o khÃ¡c. File YAML trÃªn bao gá»“m cÃ¡c thÆ° viá»‡n bá»• sung cÃ³ thá»ƒ há»¯u Ã­ch cho viá»‡c phÃ¡t triá»ƒn vÃ  debugging.

```python
import torch
import torch.nn as nn
import math

class InputEmbeddings(nn.Module):
    def __init__(self, d_model: int, vocab_size: int):
        super().__init__()
        self.d_model = d_model
        self.vocab_size = vocab_size
        # Táº¡o ma tráº­n embedding kÃ­ch thÆ°á»›c (vocab_size, d_model)
        self.embedding = nn.Embedding(vocab_size, d_model)

    def forward(self, x):
        # NhÃ¢n vá»›i cÄƒn báº­c 2 cá»§a d_model (theo bÃ i bÃ¡o) Ä‘á»ƒ scale giÃ¡ trá»‹
        # GiÃºp gradient á»•n Ä‘á»‹nh hÆ¡n khi d_model lá»›n
        return self.embedding(x) * math.sqrt(self.d_model)
```

---

## Kiáº¿n trÃºc Transformer: Tá»•ng quan chi tiáº¿t

Sau khi Ä‘Ã£ cÃ³ khÃ¡i niá»‡m cÆ¡ báº£n vá» Encoder vÃ  Decoder, hÃ£y Ä‘i sÃ¢u vÃ o kiáº¿n trÃºc Transformer má»™t cÃ¡ch chi tiáº¿t. Transformer Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn cÆ¡ cháº¿ **Attention** vÃ  **Multi-Head Attention**, loáº¡i bá» hoÃ n toÃ n viá»‡c xá»­ lÃ½ tuáº§n tá»± nhÆ° RNN.

>HÃ¬nh áº£nh: Kiáº¿n trÃºc tá»•ng quan Transformer vá»›i Encoder-Decoder

### CÃ¡c thÃ nh pháº§n chÃ­nh

1. **Input Embeddings**: Chuyá»ƒn Ä‘á»•i tá»« thÃ nh vector sá»‘ há»c (Ä‘Ã£ implement á»Ÿ trÃªn).
2. **Positional Encoding**: ThÃªm thÃ´ng tin vá»‹ trÃ­ vÃ o embeddings vÃ¬ Transformer khÃ´ng cÃ³ khÃ¡i niá»‡m thá»© tá»± tá»± nhiÃªn.
3. **Multi-Head Attention**: CÆ¡ cháº¿ chÃº Ã½ Ä‘a Ä‘áº§u, cho phÃ©p mÃ´ hÃ¬nh táº­p trung vÃ o nhiá»u pháº§n khÃ¡c nhau cá»§a input.
4. **Feed-Forward Networks**: Máº¡ng nÆ¡-ron feed-forward Ã¡p dá»¥ng cho má»—i position.
5. **Layer Normalization vÃ  Residual Connections**: Äá»ƒ á»•n Ä‘á»‹nh training vÃ  cáº£i thiá»‡n gradient flow.
6. **Encoder Stack**: N chá»“ng lÃªn nhau cá»§a Encoder layers.
7. **Decoder Stack**: TÆ°Æ¡ng tá»± nhÆ°ng cÃ³ thÃªm Masked Multi-Head Attention.

---

## Chi tiáº¿t cÃ¡c thÃ nh pháº§n vá»›i code

### 1. Positional Encoding

VÃ¬ Transformer khÃ´ng sá»­ dá»¥ng RNN, nÃ³ khÃ´ng cÃ³ thÃ´ng tin vá» thá»© tá»± cá»§a cÃ¡c tá»«. Positional Encoding giáº£i quyáº¿t váº¥n Ä‘á» nÃ y báº±ng cÃ¡ch thÃªm vector vá»‹ trÃ­ vÃ o embeddings.

>CÃ´ng thá»©c: Positional Encoding vá»›i hÃ m sin vÃ  cos

```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, seq_len: int, dropout: float):
        super().__init__()
        self.d_model = d_model
        self.seq_len = seq_len
        self.dropout = nn.Dropout(dropout)

        # Táº¡o ma tráº­n PE cÃ³ kÃ­ch thÆ°á»›c (seq_len, d_model)
        pe = torch.zeros(seq_len, d_model)

        # Táº¡o vector position tá»« 0 Ä‘áº¿n seq_len-1
        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)  # (seq_len, 1)

        # TÃ­nh div_term = 10000^(2i/d_model)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # (d_model/2,)

        # Ãp dá»¥ng sin cho vá»‹ trÃ­ cháºµn (2i), cos cho vá»‹ trÃ­ láº» (2i+1)
        pe[:, 0::2] = torch.sin(position * div_term)  # Vá»‹ trÃ­ cháºµn
        pe[:, 1::2] = torch.cos(position * div_term)  # Vá»‹ trÃ­ láº»

        # ThÃªm batch dimension vÃ  transpose thÃ nh (1, seq_len, d_model)
        pe = pe.unsqueeze(0)  # (1, seq_len, d_model)

        # ÄÄƒng kÃ½ pe nhÆ° buffer Ä‘á»ƒ khÃ´ng Ä‘Æ°á»£c update trong training
        self.register_buffer('pe', pe)

    def forward(self, x):
        # Cá»™ng positional encoding vÃ o input embeddings
        # x shape: (batch_size, seq_len, d_model)
        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)
        return self.dropout(x)
```

**Giáº£i thÃ­ch chi tiáº¿t:**
- `position`: Vector chá»©a vá»‹ trÃ­ cá»§a má»—i token (0, 1, 2, ...)
- `div_term`: TÃ­nh 10000^(2i/d_model) Ä‘á»ƒ táº¡o táº§n sá»‘ khÃ¡c nhau cho cÃ¡c chiá»u
- Sinusoidal functions (sin/cos) cho phÃ©p mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c relative positions tá»‘t hÆ¡n
- Dropout Ä‘Æ°á»£c Ã¡p dá»¥ng Ä‘á»ƒ trÃ¡nh overfitting

### 2. Multi-Head Attention

ÄÃ¢y lÃ  trÃ¡i tim cá»§a Transformer. Attention cho phÃ©p mÃ´ hÃ¬nh táº­p trung vÃ o cÃ¡c pháº§n quan trá»ng cá»§a input.

>HÃ¬nh áº£nh: CÆ¡ cháº¿ Multi-Head Attention vá»›i Scaled Dot-Product Attention

```python
class MultiHeadAttentionBlock(nn.Module):
    def __init__(self, d_model: int, h: int, dropout: float):
        super().__init__()
        self.d_model = d_model
        self.h = h  # Sá»‘ heads

        # Äáº£m báº£o d_model chia háº¿t cho h
        assert d_model % h == 0, "d_model pháº£i chia háº¿t cho sá»‘ heads h"

        self.d_k = d_model // h  # KÃ­ch thÆ°á»›c cá»§a má»—i head

        # Linear layers Ä‘á»ƒ táº¡o Q, K, V
        self.w_q = nn.Linear(d_model, d_model)  # Query
        self.w_k = nn.Linear(d_model, d_model)  # Key
        self.w_v = nn.Linear(d_model, d_model)  # Value

        # Linear layer cuá»‘i cÃ¹ng
        self.w_o = nn.Linear(d_model, d_model)  # Output projection

        self.dropout = nn.Dropout(dropout)

    @staticmethod
    def attention(query, key, value, mask, dropout: nn.Dropout):
        d_k = query.shape[-1]

        # TÃ­nh attention scores: (batch_size, h, seq_len, seq_len)
        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)

        # Ãp dá»¥ng mask náº¿u cÃ³ (Ä‘á»ƒ trÃ¡nh nhÃ¬n vÃ o future tokens trong decoder)
        if mask is not None:
            attention_scores.masked_fill_(mask == 0, -1e9)

        # TÃ­nh attention weights vá»›i softmax
        attention_scores = attention_scores.softmax(dim=-1)  # (batch_size, h, seq_len, seq_len)

        # Ãp dá»¥ng dropout
        if dropout is not None:
            attention_scores = dropout(attention_scores)

        # TÃ­nh output: (batch_size, h, seq_len, d_k)
        return (attention_scores @ value), attention_scores

    def forward(self, q, k, v, mask):
        # q, k, v shape: (batch_size, seq_len, d_model)

        # Linear transformations vÃ  reshape thÃ nh multi-head
        query = self.w_q(q)  # (batch_size, seq_len, d_model)
        key = self.w_k(k)    # (batch_size, seq_len, d_model)
        value = self.w_v(v)  # (batch_size, seq_len, d_model)

        # Reshape thÃ nh (batch_size, seq_len, h, d_k) vÃ  transpose thÃ nh (batch_size, h, seq_len, d_k)
        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)
        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)
        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)

        # TÃ­nh attention
        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)

        # Concatenate heads vÃ  Ã¡p dá»¥ng linear transformation cuá»‘i
        # x shape: (batch_size, h, seq_len, d_k) -> (batch_size, seq_len, h, d_k) -> (batch_size, seq_len, d_model)
        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)

        # Ãp dá»¥ng linear layer cuá»‘i
        return self.w_o(x)
```

**Giáº£i thÃ­ch chi tiáº¿t:**
- **Multi-Head**: Chia d_model thÃ nh h heads, má»—i head há»c attention patterns khÃ¡c nhau
- **Scaled Dot-Product Attention**: TÃ­nh Ä‘á»™ tÆ°Æ¡ng tá»± giá»¯a query vÃ  key, scale báº±ng sqrt(d_k) Ä‘á»ƒ á»•n Ä‘á»‹nh
- **Mask**: Trong decoder, mask future tokens Ä‘á»ƒ trÃ¡nh cheating
- **Output projection**: Káº¿t há»£p thÃ´ng tin tá»« táº¥t cáº£ heads

### 3. Feed-Forward Network vÃ  Residual Connections

```python
class FeedForwardBlock(nn.Module):
    def __init__(self, d_model: int, d_ff: int, dropout: float):
        super().__init__()
        self.linear_1 = nn.Linear(d_model, d_ff)  # Má»Ÿ rá»™ng dimension
        self.dropout = nn.Dropout(dropout)
        self.linear_2 = nn.Linear(d_ff, d_model)  # Thu háº¹p vá» d_model

    def forward(self, x):
        # FFN(x) = max(0, xW1 + b1)W2 + b2
        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))

class ResidualConnection(nn.Module):
    def __init__(self, features: int, dropout: float):
        super().__init__()
        self.dropout = nn.Dropout(dropout)
        self.norm = nn.LayerNorm(features)

    def forward(self, x, sublayer):
        # Residual connection: x + dropout(sublayer(norm(x)))
        return x + self.dropout(sublayer(self.norm(x)))
```

**Giáº£i thÃ­ch:**
- **Feed-Forward**: Máº¡ng 2 lá»›p vá»›i ReLU activation, giÃºp mÃ´ hÃ¬nh há»c non-linear transformations
- **Residual Connection**: Cá»™ng input vá»›i output cá»§a sublayer Ä‘á»ƒ trÃ¡nh vanishing gradients
- **Layer Norm**: Chuáº©n hÃ³a theo features Ä‘á»ƒ á»•n Ä‘á»‹nh training

---

## MÃ´ hÃ¬nh Encoder

Encoder nháº­n input sequence vÃ  táº¡o ra representations cho decoder sá»­ dá»¥ng.

>HÃ¬nh áº£nh: Cáº¥u trÃºc Encoder Block vá»›i Self-Attention vÃ  Feed-Forward Networks

```python
class EncoderBlock(nn.Module):
    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float):
        super().__init__()
        self.self_attention_block = self_attention_block
        self.feed_forward_block = feed_forward_block
        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])

    def forward(self, x, src_mask):
        # Self-attention vá»›i residual connection
        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))

        # Feed-forward vá»›i residual connection
        x = self.residual_connections[1](x, self.feed_forward_block)

        return x

class Encoder(nn.Module):
    def __init__(self, features: int, layers: nn.ModuleList):
        super().__init__()
        self.layers = layers
        self.norm = nn.LayerNorm(features)

    def forward(self, x, mask):
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)
```

**Giáº£i thÃ­ch Encoder:**
- **Encoder Block**: Bao gá»“m self-attention vÃ  feed-forward vá»›i residual connections
- **Stack of Blocks**: N layers chá»“ng lÃªn nhau, má»—i layer há»c representations phá»©c táº¡p hÆ¡n
- **Final Normalization**: Chuáº©n hÃ³a output cuá»‘i cÃ¹ng

---

## MÃ´ hÃ¬nh Decoder

Decoder sinh output sequence dá»±a trÃªn encoder output vÃ  previous output tokens.

>HÃ¬nh áº£nh: Cáº¥u trÃºc Decoder Block vá»›i Masked Self-Attention, Cross-Attention vÃ  Feed-Forward Networks

```python
class DecoderBlock(nn.Module):
    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float):
        super().__init__()
        self.self_attention_block = self_attention_block
        self.cross_attention_block = cross_attention_block
        self.feed_forward_block = feed_forward_block
        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])

    def forward(self, x, encoder_output, src_mask, tgt_mask):
        # Masked self-attention (chá»‰ nhÃ¬n vÃ o past tokens)
        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))

        # Cross-attention vá»›i encoder output
        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))

        # Feed-forward
        x = self.residual_connections[2](x, self.feed_forward_block)

        return x

class Decoder(nn.Module):
    def __init__(self, features: int, layers: nn.ModuleList):
        super().__init__()
        self.layers = layers
        self.norm = nn.LayerNorm(features)

    def forward(self, x, encoder_output, src_mask, tgt_mask):
        for layer in self.layers:
            x = layer(x, encoder_output, src_mask, tgt_mask)
        return self.norm(x)
```

**Giáº£i thÃ­ch Decoder:**
- **Masked Self-Attention**: Chá»‰ cho phÃ©p nhÃ¬n vÃ o tokens trÆ°á»›c Ä‘Ã³ (auto-regressive)
- **Cross-Attention**: ChÃº Ã½ vÃ o encoder output Ä‘á»ƒ láº¥y thÃ´ng tin ngá»¯ nghÄ©a
- **Feed-Forward**: TÆ°Æ¡ng tá»± encoder
- **Stack of Blocks**: TÆ°Æ¡ng tá»± encoder nhÆ°ng phá»©c táº¡p hÆ¡n

---

## GhÃ©p láº¡i: MÃ´ hÃ¬nh Transformer hoÃ n chá»‰nh

```python
class Transformer(nn.Module):
    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: nn.Linear):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.src_embed = src_embed
        self.tgt_embed = tgt_embed
        self.src_pos = src_pos
        self.tgt_pos = tgt_pos
        self.projection_layer = projection_layer

    def encode(self, src, src_mask):
        # Embedding + Positional Encoding
        src = self.src_embed(src)
        src = self.src_pos(src)

        # Encoder
        return self.encoder(src, src_mask)

    def decode(self, encoder_output, src_mask, tgt, tgt_mask):
        # Embedding + Positional Encoding
        tgt = self.tgt_embed(tgt)
        tgt = self.tgt_pos(tgt)

        # Decoder
        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)

    def project(self, x):
        # Dá»± Ä‘oÃ¡n token tiáº¿p theo
        return self.projection_layer(x)

# HÃ m táº¡o Transformer model
def build_transformer(src_vocab_size, tgt_vocab_size, src_seq_len, tgt_seq_len, d_model=512, N=6, h=8, dropout=0.1, d_ff=2048):
    # Táº¡o embeddings
    src_embed = InputEmbeddings(d_model, src_vocab_size)
    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)

    # Positional encoding
    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)
    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)

    # Táº¡o encoder blocks
    encoder_blocks = []
    for _ in range(N):
        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)
        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)
        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)
        encoder_blocks.append(encoder_block)

    # Táº¡o decoder blocks
    decoder_blocks = []
    for _ in range(N):
        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)
        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)
        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)
        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)
        decoder_blocks.append(decoder_block)

    # Táº¡o encoder vÃ  decoder
    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))
    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))

    # Projection layer
    projection_layer = nn.Linear(d_model, tgt_vocab_size)

    # Táº¡o transformer
    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)

    # Initialize parameters
    for p in transformer.parameters():
        if p.dim() > 1:
            nn.init.xavier_uniform_(p)

    return transformer
```

**Giáº£i thÃ­ch kiáº¿n trÃºc hoÃ n chá»‰nh:**
- **encode()**: Xá»­ lÃ½ input source qua embedding, positional encoding vÃ  encoder stack
- **decode()**: Sá»­ dá»¥ng encoder output vÃ  target sequence Ä‘á»ƒ sinh output
- **project()**: Chuyá»ƒn Ä‘á»•i decoder output thÃ nh logits cho vocabulary
- **build_transformer()**: Factory function táº¡o model vá»›i cÃ¡c hyperparameters tá»« bÃ i bÃ¡o gá»‘c

---

## ğŸ‰ ChÃºc má»«ng báº¡n Ä‘Ã£ Ä‘á»c xong bÃ i viáº¿t!
![ChÃºc má»«ng báº¡n Ä‘Ã£ Ä‘á»c xong!](/static/images/blogs/anya-chan-chuc-mung.gif)
